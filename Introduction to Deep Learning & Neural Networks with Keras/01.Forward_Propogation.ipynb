{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7efbfcd",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks - Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b8cd15",
   "metadata": {},
   "source": [
    "## Objective\n",
    "- Build a Neural Network\n",
    "- Compute Weighted Sum at Each Node (z)\n",
    "- Compute Node Activation (a)\n",
    "- Perform Forward Propogation for Propogating data\n",
    "\n",
    "<img src=\"http://cocl.us/neural_network_example\" alt=\"Neural Network Example\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba028237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900e4843",
   "metadata": {},
   "source": [
    "Let's start by randomly initializing the weights and the biases in the network. We have 6 weights and 3 biases, one for each node in the hidden layer as well as for each node in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c9e83cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights= np.around(np.random.uniform(size=6), decimals=2)\n",
    "bias= np.around(np.random.uniform(size=3), decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df77b1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [0.43 0.28 0.79 0.4  0.31 0.25]\n",
      "Bias: [0.74 0.84 0.15]\n"
     ]
    }
   ],
   "source": [
    "print(\"Weights:\", weights)\n",
    "print(\"Bias:\", bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d3b982",
   "metadata": {},
   "source": [
    "Let's compute the output for a given input, $x_1$ and $x_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09f234b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1= 0.61\n",
    "x2= 0.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4eda3850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.2487)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z11= weights[0]*x1 + weights[1]*x2 + bias[0]\n",
    "np.around(z11, decimals=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37924975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.6739)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z12= weights[2]*x1 + weights[3]*x2 +bias[1]\n",
    "np.around(z12, decimals=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14083c7",
   "metadata": {},
   "source": [
    "Assuming a sigmoid activation function, let's compute the activation of the first node, $a_{1, 1}$, in the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b58045a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7771)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a11= 1/(1 + np.exp(-z11))\n",
    "np.around(a11, decimals=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27f1a60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.6739)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a12= 1/ (1+ np.exp(-z12))\n",
    "np.around(z12, decimals=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8abae5e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.6014)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z2= weights[4]*a11 + weights[5]*a12 +bias[2]\n",
    "np.around(z2, decimals=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f71ba722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.646)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2= 1/(1+ np.exp(-z2))\n",
    "np.around(a2, decimals=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5be514",
   "metadata": {},
   "source": [
    "In order to code an automatic way of making predictions, let's generalize our network. A general network would take $n$ inputs, would have many hidden layers, each hidden layer having $m$ nodes, and would have an output layer. Although the network is showing one hidden layer, but we will code the network to have many hidden layers. Similarly, although the network shows an output layer with one node, we will code the network to have more than one node in the output layer.\n",
    "\n",
    "<img src=\"http://cocl.us/general_neural_network\" alt=\"Neural Network General\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e04d2c",
   "metadata": {},
   "source": [
    "## Build a Neural Network\n",
    "Let's start by formally defining the structure of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "000c0607",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2  # number of inputs\n",
    "num_of_hidden= 2  # number of hidden layers\n",
    "m= [2, 2] # number of nodes in each hidden layer\n",
    "output_nodes= 1  # number of nodes in the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f048ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network structure: {'layer_1': {'node_1': {'weights': array([0.96, 0.56]), 'bias': array([0.8])}, 'node_2': {'weights': array([0.43, 0.5 ]), 'bias': array([0.56])}}, 'layer_2': {'node_1': {'weights': array([0.91, 0.04]), 'bias': array([0.48])}, 'node_2': {'weights': array([0.5 , 0.88]), 'bias': array([0.28])}}, 'output': {'node_1': {'weights': array([0.67, 0.99]), 'bias': array([0.46])}}}\n"
     ]
    }
   ],
   "source": [
    "num_node_previous= n\n",
    "\n",
    "network= {}\n",
    "\n",
    "for layer in range(num_of_hidden+1):\n",
    "    if layer== num_of_hidden:\n",
    "        layer_name= \"output\"\n",
    "        nodes= output_nodes\n",
    "    else:\n",
    "        layer_name= \"layer_{}\".format(layer+1)\n",
    "        nodes= m[layer]\n",
    "\n",
    "    network[layer_name]= {}\n",
    "    for node in range(nodes):\n",
    "        node_name= \"node_{}\".format(node+1)\n",
    "        network[layer_name][node_name]= {\n",
    "            \"weights\": np.around(np.random.uniform(size=num_node_previous), decimals=2),\n",
    "            \"bias\": np.around(np.random.uniform(size=1), decimals=2)\n",
    "        }\n",
    "    \n",
    "    num_node_previous= nodes\n",
    "\n",
    "print(\"Network structure:\", network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1a46baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_network(num_inputs, num_hidden_layers, num_nodes_hidden, num_nodes_output):\n",
    "    \n",
    "    num_nodes_previous = num_inputs # number of nodes in the previous layer\n",
    "\n",
    "    network = {}\n",
    "    \n",
    "    # loop through each layer and randomly initialize the weights and biases associated with each layer\n",
    "    for layer in range(num_hidden_layers + 1):\n",
    "        \n",
    "        if layer == num_hidden_layers:\n",
    "            layer_name = 'output' # name last layer in the network output\n",
    "            num_nodes = num_nodes_output\n",
    "        else:\n",
    "            layer_name = 'layer_{}'.format(layer + 1) # otherwise give the layer a number\n",
    "            num_nodes = num_nodes_hidden[layer] \n",
    "        \n",
    "        # initialize weights and bias for each node\n",
    "        network[layer_name] = {}\n",
    "        for node in range(num_nodes):\n",
    "            node_name = 'node_{}'.format(node+1)\n",
    "            network[layer_name][node_name] = {\n",
    "                'weights': np.around(np.random.uniform(size=num_nodes_previous), decimals=2),\n",
    "                'bias': np.around(np.random.uniform(size=1), decimals=2),\n",
    "            }\n",
    "    \n",
    "        num_nodes_previous = num_nodes\n",
    "\n",
    "    return network # return the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1a30019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer_1': {'node_1': {'weights': array([0.01, 0.92, 0.9 , 0.03, 0.96]),\n",
       "   'bias': array([0.14])},\n",
       "  'node_2': {'weights': array([0.28, 0.61, 0.94, 0.85, 0.  ]),\n",
       "   'bias': array([0.52])},\n",
       "  'node_3': {'weights': array([0.55, 0.49, 0.77, 0.16, 0.76]),\n",
       "   'bias': array([0.02])}},\n",
       " 'layer_2': {'node_1': {'weights': array([0.14, 0.12, 0.31]),\n",
       "   'bias': array([0.67])},\n",
       "  'node_2': {'weights': array([0.47, 0.82, 0.29]), 'bias': array([0.73])}},\n",
       " 'layer_3': {'node_1': {'weights': array([0.7 , 0.33]), 'bias': array([0.33])},\n",
       "  'node_2': {'weights': array([0.98, 0.62]), 'bias': array([0.95])},\n",
       "  'node_3': {'weights': array([0.77, 0.83]), 'bias': array([0.41])}},\n",
       " 'output': {'node_1': {'weights': array([0.45, 0.4 , 1.  ]),\n",
       "   'bias': array([0.18])}}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_network= initialize_network(5, 3, [3, 2, 3], 1)\n",
    "test_network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed00435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weighted_sum(inputs, weights, bias):\n",
    "    return np.sum(inputs * weights) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3de9e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "np.random.seed(12)\n",
    "inputs = np.around(np.random.uniform(size=5), decimals=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a779d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0818])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_weights = test_network['layer_1']['node_1']['weights']\n",
    "node_bias = test_network['layer_1']['node_1']['bias']\n",
    "\n",
    "weighted_sum = compute_weighted_sum(inputs, node_weights, node_bias)\n",
    "weighted_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b2a48e",
   "metadata": {},
   "source": [
    "## Compute Node Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8569ee37",
   "metadata": {},
   "source": [
    "Recall that the output of each node is simply a non-linear tranformation of the weighted sum. We use activation functions for this mapping. Let's use the sigmoid function as the activation function here. So let's define a function that takes a weighted sum as input and returns the non-linear transformation of the input using the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4b4493ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_activation(weighted_sum):\n",
    "    return 1.0 / (1.0 + np.exp(-1 * weighted_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d3efe771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.74683447])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_output  = node_activation(compute_weighted_sum(inputs, node_weights, node_bias))\n",
    "node_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd6ac1f",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9199ee58",
   "metadata": {},
   "source": [
    "The final piece of building a neural network that can perform predictions is to put everything together. So let's create a function that applies the *compute_weighted_sum* and *node_activation* functions to each node in the network and propagates the data all the way to the output layer and outputs a prediction for each node in the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbc96cc",
   "metadata": {},
   "source": [
    "The way we are going to accomplish this is through the following procedure:\n",
    "\n",
    "1. Start with the input layer as the input to the first hidden layer.\n",
    "2. Compute the weighted sum at the nodes of the current layer.\n",
    "3. Compute the output of the nodes of the current layer.\n",
    "4. Set the output of the current layer to be the input to the next layer.\n",
    "5. Move to the next layer in the network.\n",
    "6. Repeat steps 2 - 5 until we compute the output of the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4860b602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(network, inputs):\n",
    "    \n",
    "    layer_inputs = list(inputs) # start with the input layer as the input to the first hidden layer\n",
    "    \n",
    "    for layer in network:\n",
    "        \n",
    "        layer_data = network[layer]\n",
    "        \n",
    "        layer_outputs = [] \n",
    "        for layer_node in layer_data:\n",
    "        \n",
    "            node_data = layer_data[layer_node]\n",
    "        \n",
    "            # compute the weighted sum and the output of each node at the same time \n",
    "            node_output = node_activation(compute_weighted_sum(layer_inputs, node_data['weights'], node_data['bias']))\n",
    "            layer_outputs.append(np.around(node_output[0], decimals=4))\n",
    "            \n",
    "        if layer != 'output':\n",
    "            print('The outputs of the nodes in hidden layer number {} is {}'.format(layer.split('_')[1], layer_outputs))\n",
    "    \n",
    "        layer_inputs = layer_outputs # set the output of this layer to be the input to next layer\n",
    "\n",
    "    network_predictions = layer_outputs\n",
    "    return network_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "43b8a956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The outputs of the nodes in hidden layer number 1 is [np.float64(0.7468), np.float64(0.8466), np.float64(0.6809)]\n",
      "The outputs of the nodes in hidden layer number 2 is [np.float64(0.7479), np.float64(0.8779)]\n",
      "The outputs of the nodes in hidden layer number 3 is [np.float64(0.7583), np.float64(0.9027), np.float64(0.8474)]\n"
     ]
    }
   ],
   "source": [
    "predictions = forward_propagate(test_network, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "faa03fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_network = initialize_network(5, 3, [2, 3, 2], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0f505909",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.around(np.random.uniform(size=5), decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "843728df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The outputs of the nodes in hidden layer number 1 is [np.float64(0.8857), np.float64(0.8889)]\n",
      "The outputs of the nodes in hidden layer number 2 is [np.float64(0.7822), np.float64(0.6965), np.float64(0.7411)]\n",
      "The outputs of the nodes in hidden layer number 3 is [np.float64(0.868), np.float64(0.881)]\n",
      "The predicted values by the network for the given input are [np.float64(0.8952), np.float64(0.8222), np.float64(0.8035)]\n"
     ]
    }
   ],
   "source": [
    "predictions = forward_propagate(my_network, inputs)\n",
    "print('The predicted values by the network for the given input are {}'.format(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea4338b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
